import random
import numpy as np
import sympy as sp
import time
from sympy.vector import CoordSys3D, Vector
from multiprocessing import Pool  
Coord = CoordSys3D('Coord')
a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z = sp.symbols('a b c d e f g h i j k l m n o p q r s t u v w x y z')

def main():
    #feel free to play around with the class
    #test scalar line integral function
    f = MultivariableFunction([x,y,z], x**2 + y**2 + z**2)
    from vectorfieldfunction import VectorFieldFunction
    r = VectorFieldFunction([t], [t*2, t, t*3])
    print(f.scalar_line_integral(r, 0, 1, verbose = True))
class MultivariableFunction:
    def __init__(self, independent_vars: list, expr: sp.Expr):
        self.independent_vars = independent_vars
        self.expr = expr
        self.numerical_expr = sp.lambdify(self.independent_vars, self.expr)
        self.gradient = MultivariableFunction.gradient(self)
        #count of independent variables to get u = [a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z] depending on the number of independent variables
        direction_vect = [sp.symbols(chr(97 + i)) for i in range(len(independent_vars))] # list of symbols
        self.directional_diff = MultivariableFunction.directional_diff(self, direction_vect)


    def scalar_line_integral(self, r: "VectorFieldFunction", start_time: float, end_time: float, verbose = False)-> int|float: # type: ignore
        #Formula: ∫_c f(x1,x2,x3,...,xn)dx = ∫f(r_x1(t), r_x2(t), r_x3(t),..., r_xn(t))||r'(t)||dt from start_time to end_time
        #where r(t) = [r_x1(t), r_x2(t), r_x3(t),..., r_xn(t)]
        #Step 1: sub x1 = r_x1(t), x2 = r_x2(t), x3 = r_x3(t),..., xn = r_xn(t) into f(x1,x2,x3,...,xn) to get f(t)
        f = MultivariableFunction(r.independent_vars, self.expr.subs({self.independent_vars[i]: r.expr[i] for i in range(len(self.independent_vars))}))
        #Step 2: compute ||r'(t)||
        r_prime = r.diff(r.independent_vars[0]) # only one independent variable is allowed for r
        norm_r_prime = r_prime.norm()
        if verbose:
            print(f'f(t)={f.expr}')
            print(f'r\'(t)={r_prime.expr}')
        return sp.integrate(f.expr * norm_r_prime.expr, (r.independent_vars[0], start_time, end_time))

    def tangent_hyperplane(self, point: list):
        '''
        THIS FUNCTION HAS NO PRACTICAL USE, ITS MAIN PURPOSE IS TO DEMONSTRATE THE USE OF THE TANGENT HYPERPLANE 
        AT A POINT ON HIGH DIMENSIONAL SPACE GENERATED BY A MULTIVARIABLE FUNCTION.
        RETURNS THE EQUATION OF THE TANGENT HYPERPLANE AT A POINT ON A MULTIVARIABLE FUNCTION. NO NUMERICAL COMPUTATION IS DONE.
        EQUATION HAS THE FORM: L(a,b,..) = f(a0,b0,..) + fa(a0,b0,..)(a-a0) + fb(a0,b0,..)(b-b0) +...+ fz(a0,b0,..)(n-n0)
        '''
        gradient = sp.derive_by_array(self.expr, self.independent_vars)
        #lambify the gradient
        gradient = sp.lambdify(self.independent_vars, gradient)
        #evaluate partial derivatives at the point: fa(a0,b0,..), fb(a0,b0,..),..,fz(a0,b0,..),..
        gradient_at_point = gradient(*point)
        tangent_hyperplane = self.expr.subs({self.independent_vars[i]: point[i] for i in range(len(self.independent_vars))}).evalf() #f(a0,b0,..)
        print(tangent_hyperplane)
        for i in range(len(self.independent_vars)):
            tangent_hyperplane += gradient_at_point[i]*(self.independent_vars[i] - point[i]) #fa(a0,b0,..)(a-a0) + fb(a0,b0,..)(b-b0) +...+ fz(a0,b0,..)(n-n0)
        return tangent_hyperplane

    def gradient(self):
        return sp.lambdify(self.independent_vars, sp.derive_by_array(self.expr, self.independent_vars))

    def directional_diff(self, direction_vect: list[float|int]):
        gradient = sp.Matrix(sp.derive_by_array(self.expr, self.independent_vars))
        normalized_dir_v = sp.Matrix(direction_vect)/(sp.DotProduct(sp.Matrix(direction_vect), sp.Matrix(direction_vect))**0.5).doit()
        return sp.lambdify(self.independent_vars + direction_vect, sp.DotProduct(gradient, normalized_dir_v ))

    def find_best_local_extrema_within_hyperspherical_bound(self, maxima_or_minima: str
                                             , step_size: float
                                             , norm_tolerance: float
                                             , hypershperical_constraint: float
                                             , density_1D: int 
                                             ,num_of_cores: int = 8
                                             ,verbose: bool = False):
        # Create a list of parameters for each set of points
        initial_points = MultivariableFunction._set_of_point_generator(len(self.independent_vars),
                                                                        hypershperical_constraint,
                                                                        density_1D, 
                                                                        verbose)
        func = MultivariableFunction(self.independent_vars, self.expr)
        params_list = [
            (maxima_or_minima, func.independent_vars, func.expr, initial_point.tolist(), step_size, norm_tolerance, hypershperical_constraint, verbose)
            for initial_point in initial_points
        ]
    
        # Use multiprocessing Pool to run the function in parallel
        with Pool(processes=num_of_cores) as pool: #8 is max number of cores my computer have
            results = pool.map(MultivariableFunction._parallel_find_local_extrema, params_list)
    
        # After parallel computation, select the best result (min or max)
        best_result = min(results, key=lambda x: x[-1]) if maxima_or_minima.lower() == "minima" else max(results, key=lambda x: x[-1])
        print(f"norm of gradient at best result: {np.linalg.norm(func.gradient(*best_result[:-1]))}")
        print(f"distance of most optimized point from origin: {np.linalg.norm(best_result[:-1])}")
        return best_result
    
    def multivar_integral(self, bounds: list[tuple[float, float]]):
        '''
        Formula: ∫∫∫...∫f(x1,x2,x3,...,xn)dV
        where f(x1,x2,x3,...,xn) is the multivariable function
        and this method is for numerical computation
        S1 : start of the first integral of a
        E1 : end of the first integral of a
        S2 : start of the second integral of b
        E2 : end of the second integral of b
        blah blah blah
        I want second arg of sp.integrate to be =( (a,S1, E1), (b,S2, E2), (c,S3, E3),... )
        '''
        integral = self.expr
        print(f"is {self.expr}")
        for i in range(len(bounds)):
            integral = sp.integrate(integral, (self.independent_vars[i], bounds[i][0], bounds[i][1]))
        print(f"integral: {integral} on {bounds}")
        return integral
    

    #HELPER FUNCTION
    def _find_local_extrema(maxima_or_minima: str, independent_vars: list, expr: sp.Expr, initial_point: list, step_size: float | int, norm_tolerance: float, hypershperical_constraint: int, verbose: bool = False):
        '''MAIN ALGORITHM FOR GRADIENT ASCENT and DESCENT'''
        func = MultivariableFunction(independent_vars, expr)
        current_point = initial_point
        if maxima_or_minima.lower() == "maxima":
            while True:
                gradient = func.gradient(*current_point)
                norm_of_gradient = np.linalg.norm(gradient)
                if verbose:
                    print(f"Gradient at {current_point}: {gradient}")
                    print(f"Norm of gradient: {norm_of_gradient}")
                if norm_of_gradient > 2:
                    current_point = current_point + (step_size*(gradient)/ norm_of_gradient)
                else:
                    current_point = current_point + step_size*(gradient)
                if np.linalg.norm(current_point) > hypershperical_constraint:
                    print("HYPERSPHERICAL CONSTRAINT TERMINATION WITH NORM = ", np.linalg.norm(current_point))
                    break 
                if np.linalg.norm(gradient) <= norm_tolerance:
                    break
        elif maxima_or_minima.lower() == "minima":
            while True:
                gradient = func.gradient(*current_point) 
                current_point = current_point - step_size*(gradient)
                norm_of_gradient = np.linalg.norm(gradient)
                if norm_of_gradient > 2:
                    current_point = current_point - (step_size*(gradient)/ norm_of_gradient)
                else:
                    current_point = current_point - step_size*(gradient)
                if np.linalg.norm(current_point) > hypershperical_constraint:
                    print("HYPERSPHERICAL CONSTRAINT TERMINATION WITH NORM = ", np.linalg.norm(current_point))
                    break 
                if np.linalg.norm(gradient) <= norm_tolerance:
                    break
        optimized_output = func.numerical_expr(*current_point)
        if verbose:
            print(f"The local {maxima_or_minima} at: {current_point}\nThe optimized output is: {optimized_output}")
        return np.array(np.append(current_point, optimized_output))

    def _set_of_point_generator(domain_dimension: int, hypershperical_constraint: int | float, density_1D: int, verbose: bool = False)->np.ndarray:
        #Generate random sets of points bounded and distributed inside a hypersquare.
        #Args:
        #    domain_dimension (int): Number of dimensions for the points.
        #    hyperspherical_radius (int | float): Radius of the hypersphere bounding the hypersquare.
        #    density_along_a_dimension (int): Number of points along each dimension.
        #    verbose (bool, optional): If True, print additional information about the generation process.
        #Returns:
        #    numpy.ndarray: Array of shape (total_points, domain_dimension) containing the generated points.
        # Calculate the side length of the hypersquare
        hypersquare_side_length = 2 * hypershperical_constraint / np.sqrt(domain_dimension)
        # Calculate the range for each dimension
        half_side_length = hypersquare_side_length / 2
        bounds = (-half_side_length, half_side_length)
        # Calculate the total number of points
        total_points = density_1D ** domain_dimension
        if verbose:
            print(f"Hypersquare side length: {hypersquare_side_length}")
            print(f"Bounds: {bounds}")
            print(f"Total number of points: {total_points}")
        # Generate points within the bounds
        points = np.array([
            [random.uniform(bounds[0], bounds[1]) for _ in range(domain_dimension)]
            for _ in range(total_points)
        ])
        if verbose:
            print("Generated points:")
            print(points)
        return points

    # Function to run in parallel (wrapper around the original `find_local_extrema` function)
    def _parallel_find_local_extrema(params):
        maxima_or_minima, independent_vars, expr, initial_point, step_size, norm_tolerance, hypershperical_constraint, verbose = params
        return MultivariableFunction._find_local_extrema(
            maxima_or_minima, independent_vars, expr, initial_point, step_size, norm_tolerance, hypershperical_constraint, verbose
        )
    




if __name__ == "__main__":
    main()
